# PPO configuration for Walker2d-v4
env_id: "Walker2d-v4"

# Training parameters
total_timesteps: 2_000_000
num_envs: 8
num_steps: 2048

# PPO hyperparameters
learning_rate: 3e-4
clip_epsilon: 0.2
epochs: 10
mini_batch_size: 64

# GAE parameters
gamma: 0.99
gae_lambda: 0.95

# Loss coefficients
value_loss_coef: 0.5
entropy_coef: 0.0  # No entropy bonus for Walker2d

# Other parameters
max_grad_norm: 0.5
normalize_advantages: true
clip_value_loss: true
value_clip_epsilon: 0.2
target_kl: 0.01

# Network architecture
hidden_dims: [256, 256]
activation: "tanh"
log_std_init: -0.5

# Environment specific
normalize: true
lr_schedule: "linear"

# Logging and checkpointing
save_freq: 200_000
eval_freq: 50_000
log_interval: 10